{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended contennt.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit each url and take a look at its source through Chrome DevTools. You'll need to identify the html tags, special class names etc. used for the html content you are expected to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are imported for you. If you prefer to use additional libraries feel free to uncomment them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "# from pprint import pprint\n",
    "# from lxml import html\n",
    "# from lxml.html import fromstring\n",
    "# import urllib.request\n",
    "# from urllib.request import urlopen\n",
    "# import random\n",
    "# import re\n",
    "# import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "res = requests.get(url)\n",
    "html = res.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "developers = soup.select('.h3 a')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Emmanuel Garcia',\n",
       " 'Hugo van Kemenade',\n",
       " 'Darío Kondratiuk',\n",
       " 'Megan Marsh',\n",
       " 'Minko Gechev',\n",
       " 'William Falcon',\n",
       " 'Yoshifumi Kawai',\n",
       " 'Mateusz Burzyński',\n",
       " 'Sam Verschueren',\n",
       " 'Whyrusleeping',\n",
       " 'Holger Rapp',\n",
       " 'Francois Zaninotto',\n",
       " 'Dylan Vann',\n",
       " 'Miek Gieben',\n",
       " 'Richard Schneeman',\n",
       " 'Alec Thomas',\n",
       " '陈帅',\n",
       " 'François Beaufort',\n",
       " 'Tanner Linsley',\n",
       " 'Erik Wijmans',\n",
       " 'Joshua Blum',\n",
       " 'Gabriel Aszalos',\n",
       " 'Rico Sta. Cruz',\n",
       " 'Solly Ross',\n",
       " 'Vinayak Mehta']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "type(developers[0].text)\n",
    "\n",
    "[e.text.strip() for e in developers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "res = requests.get(url)\n",
    "html = res.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "repos = soup.select('.h3 a')\n",
    "\n",
    "repos = [e.text.split(\"/\") for e in repos]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spleeter', 'gpt-2', 'models', 'interview_internal_reference', 'python', 'GNNs-for-NLP', 'Ultra-Light-Fast-Generic-Face-Detector-1MB', 'transformers', 'research-charnet', 'perception', 'interpy-zh', 'azure-cli', 'django-rest-framework', 'algo', 'marshmallow', 'models', 'ClamAV_0Day_exploit', 'xlnet', 'ZEN', 'DeepCTR', 'bert', 'Python', 'hydra', 'httpx', 'pointnet2']\n"
     ]
    }
   ],
   "source": [
    "repos2 = [f.strip() for e in repos for f in e]\n",
    "print(repos2[1::2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/20px-Cscr-featured.svg.png',\n",
       " 'https//upload.wikimedia.org/wikipedia/en/thumb/1/1b/Semi-protection-shackle.svg/20px-Semi-protection-shackle.svg.png',\n",
       " 'https//upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG',\n",
       " 'https//upload.wikimedia.org/wikipedia/commons/thumb/8/87/Walt_Disney_1942_signature.svg/150px-Walt_Disney_1942_signature.svg.png',\n",
       " 'https//upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg',\n",
       " 'https//upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Newman_Laugh-O-Gram_%281921%29.webm/220px-seek%3D2-Newman_Laugh-O-Gram_%281921%29.webm.jpg',\n",
       " 'https//upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Trolley_Troubles_poster.jpg/170px-Trolley_Troubles_poster.jpg',\n",
       " 'https//upload.wikimedia.org/wikipedia/commons/thumb/7/71/Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg/170px-Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg',\n",
       " 'https//upload.wikimedia.org/wikipedia/en/thumb/4/4e/Steamboat-willie.jpg/170px-Steamboat-willie.jpg',\n",
       " 'https//upload.wikimedia.org/wikipedia/commons/thumb/5/57/Walt_Disney_1935.jpg/170px-Walt_Disney_1935.jpg',\n",
       " 'https//upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg/220px-Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg',\n",
       " 'https//upload.wikimedia.org/wikipedia/commons/thumb/1/15/Disney_drawing_goofy.jpg/170px-Disney_drawing_goofy.jpg',\n",
       " 'https//upload.wikimedia.org/wikipedia/commons/thumb/1/13/DisneySchiphol1951.jpg/220px-DisneySchiphol1951.jpg',\n",
       " 'https//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/WaltDisneyplansDisneylandDec1954.jpg/220px-WaltDisneyplansDisneylandDec1954.jpg',\n",
       " 'https//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Walt_disney_portrait_right.jpg/170px-Walt_disney_portrait_right.jpg',\n",
       " 'https//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Walt_Disney_Grave.JPG/170px-Walt_Disney_Grave.JPG',\n",
       " 'https//upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Roy_O._Disney_with_Company_at_Press_Conference.jpg/170px-Roy_O._Disney_with_Company_at_Press_Conference.jpg',\n",
       " 'https//upload.wikimedia.org/wikipedia/commons/thumb/a/a9/Disney_Display_Case.JPG/170px-Disney_Display_Case.JPG',\n",
       " 'https//upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Disney1968.jpg/170px-Disney1968.jpg',\n",
       " 'https//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n",
       " 'https//upload.wikimedia.org/wikipedia/commons/thumb/d/da/Animation_disc.svg/30px-Animation_disc.svg.png',\n",
       " 'https//upload.wikimedia.org/wikipedia/en/thumb/6/69/P_vip.svg/29px-P_vip.svg.png',\n",
       " 'https//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Magic_Kingdom_castle.jpg/24px-Magic_Kingdom_castle.jpg',\n",
       " 'https//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Video-x-generic.svg/30px-Video-x-generic.svg.png',\n",
       " 'https//upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Flag_of_Los_Angeles_County%2C_California.svg/30px-Flag_of_Los_Angeles_County%2C_California.svg.png',\n",
       " 'https//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Blank_television_set.svg/30px-Blank_television_set.svg.png',\n",
       " 'https//upload.wikimedia.org/wikipedia/en/thumb/a/a4/Flag_of_the_United_States.svg/30px-Flag_of_the_United_States.svg.png',\n",
       " 'https//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/22px-Commons-logo.svg.png',\n",
       " 'https//upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/25px-Wikiquote-logo.svg.png',\n",
       " 'https//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Wikidata-logo.svg/30px-Wikidata-logo.svg.png',\n",
       " 'https//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n",
       " 'https//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1',\n",
       " 'https/static/images/wikimedia-button.png',\n",
       " 'https/static/images/poweredby_mediawiki_88x31.png']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#your code\n",
    "\n",
    "res = requests.get(url)\n",
    "html = res.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "images = soup.findAll('img')\n",
    "images = [\"https\"+e.get(\"src\") for e in images]\n",
    "display(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://en.wiktionary.org/wiki/Python',\n",
       " 'https://en.wiktionary.org/wiki/python',\n",
       " '/wiki/Pythonidae',\n",
       " '/wiki/Python_(genus)',\n",
       " '/wiki/Python_(mythology)',\n",
       " '/wiki/Python_of_Aenus',\n",
       " '/wiki/Python_(painter)',\n",
       " '/wiki/Python_of_Byzantium',\n",
       " '/wiki/Python_of_Catana',\n",
       " '/wiki/Python_(film)',\n",
       " '/wiki/Pythons_2',\n",
       " '/wiki/Monty_Python',\n",
       " '/wiki/Python_(Monty)_Pictures',\n",
       " '/wiki/Python_(programming_language)',\n",
       " '/wiki/CPython',\n",
       " '/wiki/CMU_Common_Lisp',\n",
       " '/wiki/PERQ#PERQ_3',\n",
       " '/wiki/Python_(Busch_Gardens_Tampa_Bay)',\n",
       " '/wiki/Python_(Coney_Island,_Cincinnati,_Ohio)',\n",
       " '/wiki/Python_(Efteling)',\n",
       " '/wiki/Python_(automobile_maker)',\n",
       " '/wiki/Python_(Ford_prototype)',\n",
       " '/wiki/Colt_Python',\n",
       " '/wiki/Python_(missile)',\n",
       " '/wiki/Python_(nuclear_primary)',\n",
       " '/wiki/Python_Anghelo',\n",
       " '/wiki/PYTHON',\n",
       " '/wiki/Cython',\n",
       " '/wiki/Pyton',\n",
       " '/wiki/File:Disambig_gray.svg',\n",
       " '/wiki/Help:Disambiguation',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Special:WhatLinksHere/Python&namespace=0',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Python&oldid=924004650',\n",
       " '/wiki/Help:Category',\n",
       " '/wiki/Category:Disambiguation_pages',\n",
       " '/wiki/Category:Disambiguation_pages_with_short_description',\n",
       " '/wiki/Category:All_article_disambiguation_pages',\n",
       " '/wiki/Category:All_disambiguation_pages',\n",
       " '/wiki/Category:Animal_common_name_disambiguation_pages',\n",
       " '/wiki/Special:MyTalk',\n",
       " '/wiki/Special:MyContributions',\n",
       " '/wiki/Python',\n",
       " '/wiki/Talk:Python',\n",
       " '/wiki/Python',\n",
       " '/wiki/Main_Page',\n",
       " '/wiki/Main_Page',\n",
       " '/wiki/Portal:Contents',\n",
       " '/wiki/Portal:Featured_content',\n",
       " '/wiki/Portal:Current_events',\n",
       " '/wiki/Special:Random',\n",
       " 'https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en',\n",
       " '//shop.wikimedia.org',\n",
       " '/wiki/Help:Contents',\n",
       " '/wiki/Wikipedia:About',\n",
       " '/wiki/Wikipedia:Community_portal',\n",
       " '/wiki/Special:RecentChanges',\n",
       " '//en.wikipedia.org/wiki/Wikipedia:Contact_us',\n",
       " '/wiki/Special:WhatLinksHere/Python',\n",
       " '/wiki/Special:RecentChangesLinked/Python',\n",
       " '/wiki/Wikipedia:File_Upload_Wizard',\n",
       " '/wiki/Special:SpecialPages',\n",
       " 'https://www.wikidata.org/wiki/Special:EntityPage/Q747452',\n",
       " 'https://commons.wikimedia.org/wiki/Category:Python',\n",
       " 'https://af.wikipedia.org/wiki/Python',\n",
       " 'https://als.wikipedia.org/wiki/Python',\n",
       " 'https://az.wikipedia.org/wiki/Python',\n",
       " 'https://bn.wikipedia.org/wiki/%E0%A6%AA%E0%A6%BE%E0%A6%87%E0%A6%A5%E0%A6%A8_(%E0%A6%A6%E0%A7%8D%E0%A6%AC%E0%A7%8D%E0%A6%AF%E0%A6%B0%E0%A7%8D%E0%A6%A5%E0%A6%A4%E0%A6%BE_%E0%A6%A8%E0%A6%BF%E0%A6%B0%E0%A6%B8%E0%A6%A8)',\n",
       " 'https://be.wikipedia.org/wiki/Python',\n",
       " 'https://bg.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%BF%D0%BE%D1%8F%D1%81%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5)',\n",
       " 'https://cs.wikipedia.org/wiki/Python_(rozcestn%C3%ADk)',\n",
       " 'https://da.wikipedia.org/wiki/Python',\n",
       " 'https://de.wikipedia.org/wiki/Python',\n",
       " 'https://eo.wikipedia.org/wiki/Pitono_(apartigilo)',\n",
       " 'https://eu.wikipedia.org/wiki/Python_(argipena)',\n",
       " 'https://fa.wikipedia.org/wiki/%D9%BE%D8%A7%DB%8C%D8%AA%D9%88%D9%86',\n",
       " 'https://fr.wikipedia.org/wiki/Python',\n",
       " 'https://ko.wikipedia.org/wiki/%ED%8C%8C%EC%9D%B4%EC%84%A0',\n",
       " 'https://hr.wikipedia.org/wiki/Python_(razdvojba)',\n",
       " 'https://io.wikipedia.org/wiki/Pitono',\n",
       " 'https://id.wikipedia.org/wiki/Python',\n",
       " 'https://ia.wikipedia.org/wiki/Python_(disambiguation)',\n",
       " 'https://is.wikipedia.org/wiki/Python_(a%C3%B0greining)',\n",
       " 'https://it.wikipedia.org/wiki/Python_(disambigua)',\n",
       " 'https://he.wikipedia.org/wiki/%D7%A4%D7%99%D7%AA%D7%95%D7%9F',\n",
       " 'https://ka.wikipedia.org/wiki/%E1%83%9E%E1%83%98%E1%83%97%E1%83%9D%E1%83%9C%E1%83%98_(%E1%83%9B%E1%83%A0%E1%83%90%E1%83%95%E1%83%90%E1%83%9A%E1%83%9B%E1%83%9C%E1%83%98%E1%83%A8%E1%83%95%E1%83%9C%E1%83%94%E1%83%9A%E1%83%9D%E1%83%95%E1%83%90%E1%83%9C%E1%83%98)',\n",
       " 'https://kg.wikipedia.org/wiki/Mboma_(nyoka)',\n",
       " 'https://la.wikipedia.org/wiki/Python_(discretiva)',\n",
       " 'https://lb.wikipedia.org/wiki/Python',\n",
       " 'https://hu.wikipedia.org/wiki/Python_(egy%C3%A9rtelm%C5%B1s%C3%ADt%C5%91_lap)',\n",
       " 'https://mr.wikipedia.org/wiki/%E0%A4%AA%E0%A4%BE%E0%A4%AF%E0%A4%A5%E0%A5%89%E0%A4%A8_(%E0%A4%86%E0%A4%9C%E0%A5%8D%E0%A4%9E%E0%A4%BE%E0%A4%B5%E0%A4%B2%E0%A5%80_%E0%A4%AD%E0%A4%BE%E0%A4%B7%E0%A4%BE)',\n",
       " 'https://nl.wikipedia.org/wiki/Python',\n",
       " 'https://ja.wikipedia.org/wiki/%E3%83%91%E3%82%A4%E3%82%BD%E3%83%B3',\n",
       " 'https://no.wikipedia.org/wiki/Pyton',\n",
       " 'https://pl.wikipedia.org/wiki/Pyton',\n",
       " 'https://pt.wikipedia.org/wiki/Python_(desambigua%C3%A7%C3%A3o)',\n",
       " 'https://ru.wikipedia.org/wiki/Python_(%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D1%8F)',\n",
       " 'https://sd.wikipedia.org/wiki/%D8%A7%D8%B1%DA%99',\n",
       " 'https://sk.wikipedia.org/wiki/Python',\n",
       " 'https://sr.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%B2%D0%B8%D1%88%D0%B5%D0%B7%D0%BD%D0%B0%D1%87%D0%BD%D0%B0_%D0%BE%D0%B4%D1%80%D0%B5%D0%B4%D0%BD%D0%B8%D1%86%D0%B0)',\n",
       " 'https://sh.wikipedia.org/wiki/Python',\n",
       " 'https://fi.wikipedia.org/wiki/Python',\n",
       " 'https://sv.wikipedia.org/wiki/Pyton',\n",
       " 'https://th.wikipedia.org/wiki/%E0%B9%84%E0%B8%9E%E0%B8%97%E0%B8%AD%E0%B8%99',\n",
       " 'https://tr.wikipedia.org/wiki/Python',\n",
       " 'https://uk.wikipedia.org/wiki/%D0%9F%D1%96%D1%84%D0%BE%D0%BD',\n",
       " 'https://ur.wikipedia.org/wiki/%D9%BE%D8%A7%D8%A6%DB%8C%D8%AA%DA%BE%D9%88%D9%86',\n",
       " 'https://vi.wikipedia.org/wiki/Python',\n",
       " 'https://zh.wikipedia.org/wiki/Python_(%E6%B6%88%E6%AD%A7%E4%B9%89)',\n",
       " 'https://www.wikidata.org/wiki/Special:EntityPage/Q747452#sitelinks-wikipedia',\n",
       " '//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License',\n",
       " '//foundation.wikimedia.org/wiki/Terms_of_Use',\n",
       " '//foundation.wikimedia.org/wiki/Privacy_policy',\n",
       " '//www.wikimediafoundation.org/',\n",
       " 'https://foundation.wikimedia.org/wiki/Privacy_policy',\n",
       " '/wiki/Wikipedia:About',\n",
       " '/wiki/Wikipedia:General_disclaimer',\n",
       " '//en.wikipedia.org/wiki/Wikipedia:Contact_us',\n",
       " 'https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute',\n",
       " 'https://foundation.wikimedia.org/wiki/Cookie_statement',\n",
       " '//en.m.wikipedia.org/w/index.php?title=Python&mobileaction=toggle_view_mobile',\n",
       " 'https://wikimediafoundation.org/',\n",
       " 'https://www.mediawiki.org/']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#your code\n",
    "res = requests.get(url)\n",
    "html = res.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "links = soup.findAll('a', href=True)\n",
    "links = [e.get(\"href\") for e in links if \"wiki\" in e.get(\"href\")]\n",
    "display(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Titles that have changed in the United States Code since its last release point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2 titles have changed since last release point.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#your code\n",
    "res = requests.get(url)\n",
    "html = res.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "changed = soup.findAll(class_=\"usctitlechanged\")\n",
    "display(str(len(changed))+\" titles have changed since last release point.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Python list with the top ten FBI's Most Wanted names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.fbi.gov/wanted/topten'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['JASON DEREK BROWN',\n",
       " 'ALEXIS FLORES',\n",
       " 'EUGENE PALMER',\n",
       " 'SANTIAGO VILLALBA MEDEROS',\n",
       " 'RAFAEL CARO-QUINTERO',\n",
       " 'ROBERT WILLIAM FISHER',\n",
       " 'BHADRESHKUMAR CHETANBHAI PATEL',\n",
       " 'ARNOLDO JIMENEZ',\n",
       " 'ALEJANDRO ROSALES CASTILLO',\n",
       " 'YASER ABDEL SAID']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code \n",
    "res = requests.get(url)\n",
    "html = res.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "fbi = soup.find_all(\"h3\", \"title\")\n",
    "[e.text.strip() for e in fbi]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>latitude</th>\n",
       "      <th>latitude2</th>\n",
       "      <th>longitude</th>\n",
       "      <th>longitude2</th>\n",
       "      <th>region_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-11-0718:56:03.820</td>\n",
       "      <td>35.72</td>\n",
       "      <td>N</td>\n",
       "      <td>121.06</td>\n",
       "      <td>W</td>\n",
       "      <td>CENTRAL CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-11-0718:52:18.124</td>\n",
       "      <td>37.89</td>\n",
       "      <td>N</td>\n",
       "      <td>112.48</td>\n",
       "      <td>W</td>\n",
       "      <td>UTAH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-11-0718:38:27.438</td>\n",
       "      <td>46.29</td>\n",
       "      <td>N</td>\n",
       "      <td>7.37</td>\n",
       "      <td>E</td>\n",
       "      <td>SWITZERLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-11-0718:35:26.241</td>\n",
       "      <td>46.32</td>\n",
       "      <td>N</td>\n",
       "      <td>7.35</td>\n",
       "      <td>E</td>\n",
       "      <td>SWITZERLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-11-0718:25:00.051</td>\n",
       "      <td>16.36</td>\n",
       "      <td>S</td>\n",
       "      <td>72.82</td>\n",
       "      <td>W</td>\n",
       "      <td>NEAR COAST OF SOUTHERN PERU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-11-0718:21:12.755</td>\n",
       "      <td>19.14</td>\n",
       "      <td>N</td>\n",
       "      <td>155.64</td>\n",
       "      <td>W</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019-11-0718:16:30.01hr 00</td>\n",
       "      <td>11.30</td>\n",
       "      <td>N</td>\n",
       "      <td>86.32</td>\n",
       "      <td>W</td>\n",
       "      <td>NEAR COAST OF NICARAGUA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019-11-0718:00:57.61hr 15</td>\n",
       "      <td>27.03</td>\n",
       "      <td>N</td>\n",
       "      <td>55.05</td>\n",
       "      <td>E</td>\n",
       "      <td>SOUTHERN IRAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019-11-0717:59:30.01hr 17</td>\n",
       "      <td>8.64</td>\n",
       "      <td>N</td>\n",
       "      <td>82.81</td>\n",
       "      <td>W</td>\n",
       "      <td>PANAMA-COSTA RICA BORDER REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019-11-0717:54:30.01hr 22</td>\n",
       "      <td>11.49</td>\n",
       "      <td>N</td>\n",
       "      <td>85.98</td>\n",
       "      <td>W</td>\n",
       "      <td>NICARAGUA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2019-11-0717:52:17.61hr 24</td>\n",
       "      <td>34.27</td>\n",
       "      <td>N</td>\n",
       "      <td>119.29</td>\n",
       "      <td>W</td>\n",
       "      <td>SANTA BARBARA CHANNEL, CALIF.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2019-11-0717:48:50.61hr 27</td>\n",
       "      <td>27.29</td>\n",
       "      <td>N</td>\n",
       "      <td>55.02</td>\n",
       "      <td>E</td>\n",
       "      <td>SOUTHERN IRAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2019-11-0717:47:53.21hr 28</td>\n",
       "      <td>41.77</td>\n",
       "      <td>N</td>\n",
       "      <td>13.61</td>\n",
       "      <td>E</td>\n",
       "      <td>SOUTHERN ITALY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2019-11-0717:35:21.71hr 41</td>\n",
       "      <td>41.78</td>\n",
       "      <td>N</td>\n",
       "      <td>13.61</td>\n",
       "      <td>E</td>\n",
       "      <td>SOUTHERN ITALY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2019-11-0717:29:58.11hr 46</td>\n",
       "      <td>12.85</td>\n",
       "      <td>S</td>\n",
       "      <td>45.45</td>\n",
       "      <td>E</td>\n",
       "      <td>MAYOTTE REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2019-11-0717:18:00.01hr 58</td>\n",
       "      <td>12.41</td>\n",
       "      <td>S</td>\n",
       "      <td>77.18</td>\n",
       "      <td>W</td>\n",
       "      <td>NEAR COAST OF CENTRAL PERU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2019-11-0717:11:12.72hr 05</td>\n",
       "      <td>37.77</td>\n",
       "      <td>N</td>\n",
       "      <td>21.99</td>\n",
       "      <td>E</td>\n",
       "      <td>SOUTHERN GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2019-11-0716:50:25.02hr 26</td>\n",
       "      <td>31.78</td>\n",
       "      <td>S</td>\n",
       "      <td>71.30</td>\n",
       "      <td>W</td>\n",
       "      <td>COQUIMBO, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2019-11-0716:24:29.12hr 52</td>\n",
       "      <td>39.14</td>\n",
       "      <td>N</td>\n",
       "      <td>20.61</td>\n",
       "      <td>E</td>\n",
       "      <td>GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2019-11-0715:40:53.73hr 35</td>\n",
       "      <td>43.93</td>\n",
       "      <td>N</td>\n",
       "      <td>11.59</td>\n",
       "      <td>E</td>\n",
       "      <td>CENTRAL ITALY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2019-11-0715:40:09.03hr 36</td>\n",
       "      <td>31.37</td>\n",
       "      <td>S</td>\n",
       "      <td>68.55</td>\n",
       "      <td>W</td>\n",
       "      <td>SAN JUAN, ARGENTINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2019-11-0715:14:49.04hr 01</td>\n",
       "      <td>21.65</td>\n",
       "      <td>S</td>\n",
       "      <td>68.63</td>\n",
       "      <td>W</td>\n",
       "      <td>ANTOFAGASTA, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2019-11-0715:08:11.64hr 08</td>\n",
       "      <td>42.72</td>\n",
       "      <td>N</td>\n",
       "      <td>13.23</td>\n",
       "      <td>E</td>\n",
       "      <td>CENTRAL ITALY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2019-11-0715:01:10.84hr 15</td>\n",
       "      <td>42.73</td>\n",
       "      <td>N</td>\n",
       "      <td>13.25</td>\n",
       "      <td>E</td>\n",
       "      <td>CENTRAL ITALY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2019-11-0714:54:15.54hr 22</td>\n",
       "      <td>34.27</td>\n",
       "      <td>N</td>\n",
       "      <td>119.29</td>\n",
       "      <td>W</td>\n",
       "      <td>SANTA BARBARA CHANNEL, CALIF.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2019-11-0714:52:50.54hr 23</td>\n",
       "      <td>34.27</td>\n",
       "      <td>N</td>\n",
       "      <td>119.29</td>\n",
       "      <td>W</td>\n",
       "      <td>SANTA BARBARA CHANNEL, CALIF.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2019-11-0714:37:54.34hr 38</td>\n",
       "      <td>19.12</td>\n",
       "      <td>N</td>\n",
       "      <td>67.24</td>\n",
       "      <td>W</td>\n",
       "      <td>PUERTO RICO REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2019-11-0714:36:33.34hr 40</td>\n",
       "      <td>27.15</td>\n",
       "      <td>N</td>\n",
       "      <td>55.04</td>\n",
       "      <td>E</td>\n",
       "      <td>SOUTHERN IRAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2019-11-0714:33:33.74hr 43</td>\n",
       "      <td>34.28</td>\n",
       "      <td>N</td>\n",
       "      <td>119.29</td>\n",
       "      <td>W</td>\n",
       "      <td>GREATER LOS ANGELES AREA, CALIF.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2019-11-0714:33:33.04hr 43</td>\n",
       "      <td>19.80</td>\n",
       "      <td>N</td>\n",
       "      <td>69.83</td>\n",
       "      <td>W</td>\n",
       "      <td>DOMINICAN REPUBLIC REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2019-11-0714:21:48.64hr 54</td>\n",
       "      <td>3.98</td>\n",
       "      <td>S</td>\n",
       "      <td>104.01</td>\n",
       "      <td>W</td>\n",
       "      <td>CENTRAL EAST PACIFIC RISE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2019-11-0713:34:38.15hr 42</td>\n",
       "      <td>59.02</td>\n",
       "      <td>N</td>\n",
       "      <td>152.18</td>\n",
       "      <td>W</td>\n",
       "      <td>SOUTHERN ALASKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2019-11-0713:30:57.85hr 45</td>\n",
       "      <td>34.27</td>\n",
       "      <td>N</td>\n",
       "      <td>119.30</td>\n",
       "      <td>W</td>\n",
       "      <td>SANTA BARBARA CHANNEL, CALIF.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2019-11-0713:28:52.75hr 47</td>\n",
       "      <td>36.69</td>\n",
       "      <td>N</td>\n",
       "      <td>98.44</td>\n",
       "      <td>W</td>\n",
       "      <td>OKLAHOMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2019-11-0713:25:35.35hr 51</td>\n",
       "      <td>46.26</td>\n",
       "      <td>N</td>\n",
       "      <td>7.01</td>\n",
       "      <td>E</td>\n",
       "      <td>SWITZERLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2019-11-0713:19:44.85hr 56</td>\n",
       "      <td>34.27</td>\n",
       "      <td>N</td>\n",
       "      <td>119.29</td>\n",
       "      <td>W</td>\n",
       "      <td>SANTA BARBARA CHANNEL, CALIF.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2019-11-0713:16:11.06hr 00</td>\n",
       "      <td>12.97</td>\n",
       "      <td>N</td>\n",
       "      <td>89.06</td>\n",
       "      <td>W</td>\n",
       "      <td>OFFSHORE EL SALVADOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2019-11-0713:07:32.26hr 09</td>\n",
       "      <td>29.17</td>\n",
       "      <td>S</td>\n",
       "      <td>12.87</td>\n",
       "      <td>W</td>\n",
       "      <td>SOUTHERN MID-ATLANTIC RIDGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2019-11-0713:05:39.46hr 11</td>\n",
       "      <td>34.28</td>\n",
       "      <td>N</td>\n",
       "      <td>119.29</td>\n",
       "      <td>W</td>\n",
       "      <td>GREATER LOS ANGELES AREA, CALIF.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2019-11-0712:59:43.36hr 16</td>\n",
       "      <td>38.13</td>\n",
       "      <td>N</td>\n",
       "      <td>38.77</td>\n",
       "      <td>E</td>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2019-11-0712:58:41.66hr 18</td>\n",
       "      <td>34.27</td>\n",
       "      <td>N</td>\n",
       "      <td>119.29</td>\n",
       "      <td>W</td>\n",
       "      <td>SANTA BARBARA CHANNEL, CALIF.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2019-11-0712:50:40.06hr 26</td>\n",
       "      <td>9.86</td>\n",
       "      <td>N</td>\n",
       "      <td>126.28</td>\n",
       "      <td>E</td>\n",
       "      <td>MINDANAO, PHILIPPINES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2019-11-0712:38:59.66hr 37</td>\n",
       "      <td>35.75</td>\n",
       "      <td>N</td>\n",
       "      <td>117.59</td>\n",
       "      <td>W</td>\n",
       "      <td>SOUTHERN CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2019-11-0712:02:16.77hr 14</td>\n",
       "      <td>19.16</td>\n",
       "      <td>N</td>\n",
       "      <td>155.46</td>\n",
       "      <td>W</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2019-11-0711:58:33.97hr 18</td>\n",
       "      <td>46.76</td>\n",
       "      <td>N</td>\n",
       "      <td>10.17</td>\n",
       "      <td>E</td>\n",
       "      <td>SWITZERLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2019-11-0711:06:50.08hr 09</td>\n",
       "      <td>18.17</td>\n",
       "      <td>S</td>\n",
       "      <td>69.69</td>\n",
       "      <td>W</td>\n",
       "      <td>TARAPACA, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2019-11-0711:02:12.08hr 14</td>\n",
       "      <td>35.22</td>\n",
       "      <td>S</td>\n",
       "      <td>72.41</td>\n",
       "      <td>W</td>\n",
       "      <td>OFFSHORE MAULE, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2019-11-0711:01:51.38hr 14</td>\n",
       "      <td>45.77</td>\n",
       "      <td>N</td>\n",
       "      <td>26.82</td>\n",
       "      <td>E</td>\n",
       "      <td>ROMANIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2019-11-0711:00:29.58hr 16</td>\n",
       "      <td>19.16</td>\n",
       "      <td>N</td>\n",
       "      <td>155.47</td>\n",
       "      <td>W</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2019-11-0710:48:21.38hr 28</td>\n",
       "      <td>47.54</td>\n",
       "      <td>N</td>\n",
       "      <td>8.18</td>\n",
       "      <td>E</td>\n",
       "      <td>SWITZERLAND</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      datetime latitude latitude2 longitude longitude2  \\\n",
       "0       2019-11-0718:56:03.820    35.72         N    121.06          W   \n",
       "1       2019-11-0718:52:18.124    37.89         N    112.48          W   \n",
       "2       2019-11-0718:38:27.438    46.29         N      7.37          E   \n",
       "3       2019-11-0718:35:26.241    46.32         N      7.35          E   \n",
       "4       2019-11-0718:25:00.051    16.36         S     72.82          W   \n",
       "5       2019-11-0718:21:12.755    19.14         N    155.64          W   \n",
       "6   2019-11-0718:16:30.01hr 00    11.30         N     86.32          W   \n",
       "7   2019-11-0718:00:57.61hr 15    27.03         N     55.05          E   \n",
       "8   2019-11-0717:59:30.01hr 17     8.64         N     82.81          W   \n",
       "9   2019-11-0717:54:30.01hr 22    11.49         N     85.98          W   \n",
       "10  2019-11-0717:52:17.61hr 24    34.27         N    119.29          W   \n",
       "11  2019-11-0717:48:50.61hr 27    27.29         N     55.02          E   \n",
       "12  2019-11-0717:47:53.21hr 28    41.77         N     13.61          E   \n",
       "13  2019-11-0717:35:21.71hr 41    41.78         N     13.61          E   \n",
       "14  2019-11-0717:29:58.11hr 46    12.85         S     45.45          E   \n",
       "15  2019-11-0717:18:00.01hr 58    12.41         S     77.18          W   \n",
       "16  2019-11-0717:11:12.72hr 05    37.77         N     21.99          E   \n",
       "17  2019-11-0716:50:25.02hr 26    31.78         S     71.30          W   \n",
       "18  2019-11-0716:24:29.12hr 52    39.14         N     20.61          E   \n",
       "19  2019-11-0715:40:53.73hr 35    43.93         N     11.59          E   \n",
       "20  2019-11-0715:40:09.03hr 36    31.37         S     68.55          W   \n",
       "21  2019-11-0715:14:49.04hr 01    21.65         S     68.63          W   \n",
       "22  2019-11-0715:08:11.64hr 08    42.72         N     13.23          E   \n",
       "23  2019-11-0715:01:10.84hr 15    42.73         N     13.25          E   \n",
       "24  2019-11-0714:54:15.54hr 22    34.27         N    119.29          W   \n",
       "25  2019-11-0714:52:50.54hr 23    34.27         N    119.29          W   \n",
       "26  2019-11-0714:37:54.34hr 38    19.12         N     67.24          W   \n",
       "27  2019-11-0714:36:33.34hr 40    27.15         N     55.04          E   \n",
       "28  2019-11-0714:33:33.74hr 43    34.28         N    119.29          W   \n",
       "29  2019-11-0714:33:33.04hr 43    19.80         N     69.83          W   \n",
       "30  2019-11-0714:21:48.64hr 54     3.98         S    104.01          W   \n",
       "31  2019-11-0713:34:38.15hr 42    59.02         N    152.18          W   \n",
       "32  2019-11-0713:30:57.85hr 45    34.27         N    119.30          W   \n",
       "33  2019-11-0713:28:52.75hr 47    36.69         N     98.44          W   \n",
       "34  2019-11-0713:25:35.35hr 51    46.26         N      7.01          E   \n",
       "35  2019-11-0713:19:44.85hr 56    34.27         N    119.29          W   \n",
       "36  2019-11-0713:16:11.06hr 00    12.97         N     89.06          W   \n",
       "37  2019-11-0713:07:32.26hr 09    29.17         S     12.87          W   \n",
       "38  2019-11-0713:05:39.46hr 11    34.28         N    119.29          W   \n",
       "39  2019-11-0712:59:43.36hr 16    38.13         N     38.77          E   \n",
       "40  2019-11-0712:58:41.66hr 18    34.27         N    119.29          W   \n",
       "41  2019-11-0712:50:40.06hr 26     9.86         N    126.28          E   \n",
       "42  2019-11-0712:38:59.66hr 37    35.75         N    117.59          W   \n",
       "43  2019-11-0712:02:16.77hr 14    19.16         N    155.46          W   \n",
       "44  2019-11-0711:58:33.97hr 18    46.76         N     10.17          E   \n",
       "45  2019-11-0711:06:50.08hr 09    18.17         S     69.69          W   \n",
       "46  2019-11-0711:02:12.08hr 14    35.22         S     72.41          W   \n",
       "47  2019-11-0711:01:51.38hr 14    45.77         N     26.82          E   \n",
       "48  2019-11-0711:00:29.58hr 16    19.16         N    155.47          W   \n",
       "49  2019-11-0710:48:21.38hr 28    47.54         N      8.18          E   \n",
       "\n",
       "                         region_name  \n",
       "0                 CENTRAL CALIFORNIA  \n",
       "1                               UTAH  \n",
       "2                        SWITZERLAND  \n",
       "3                        SWITZERLAND  \n",
       "4        NEAR COAST OF SOUTHERN PERU  \n",
       "5           ISLAND OF HAWAII, HAWAII  \n",
       "6            NEAR COAST OF NICARAGUA  \n",
       "7                      SOUTHERN IRAN  \n",
       "8    PANAMA-COSTA RICA BORDER REGION  \n",
       "9                          NICARAGUA  \n",
       "10     SANTA BARBARA CHANNEL, CALIF.  \n",
       "11                     SOUTHERN IRAN  \n",
       "12                    SOUTHERN ITALY  \n",
       "13                    SOUTHERN ITALY  \n",
       "14                    MAYOTTE REGION  \n",
       "15        NEAR COAST OF CENTRAL PERU  \n",
       "16                   SOUTHERN GREECE  \n",
       "17                   COQUIMBO, CHILE  \n",
       "18                            GREECE  \n",
       "19                     CENTRAL ITALY  \n",
       "20               SAN JUAN, ARGENTINA  \n",
       "21                ANTOFAGASTA, CHILE  \n",
       "22                     CENTRAL ITALY  \n",
       "23                     CENTRAL ITALY  \n",
       "24     SANTA BARBARA CHANNEL, CALIF.  \n",
       "25     SANTA BARBARA CHANNEL, CALIF.  \n",
       "26                PUERTO RICO REGION  \n",
       "27                     SOUTHERN IRAN  \n",
       "28  GREATER LOS ANGELES AREA, CALIF.  \n",
       "29         DOMINICAN REPUBLIC REGION  \n",
       "30         CENTRAL EAST PACIFIC RISE  \n",
       "31                   SOUTHERN ALASKA  \n",
       "32     SANTA BARBARA CHANNEL, CALIF.  \n",
       "33                          OKLAHOMA  \n",
       "34                       SWITZERLAND  \n",
       "35     SANTA BARBARA CHANNEL, CALIF.  \n",
       "36              OFFSHORE EL SALVADOR  \n",
       "37       SOUTHERN MID-ATLANTIC RIDGE  \n",
       "38  GREATER LOS ANGELES AREA, CALIF.  \n",
       "39                    EASTERN TURKEY  \n",
       "40     SANTA BARBARA CHANNEL, CALIF.  \n",
       "41             MINDANAO, PHILIPPINES  \n",
       "42               SOUTHERN CALIFORNIA  \n",
       "43          ISLAND OF HAWAII, HAWAII  \n",
       "44                       SWITZERLAND  \n",
       "45                   TARAPACA, CHILE  \n",
       "46             OFFSHORE MAULE, CHILE  \n",
       "47                           ROMANIA  \n",
       "48          ISLAND OF HAWAII, HAWAII  \n",
       "49                       SWITZERLAND  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#your code\n",
    "res = requests.get(url)\n",
    "html = res.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "data = soup.select(\"tbody tr\")\n",
    "ids = [e.get(\"id\") for e in data]\n",
    "attributes = []\n",
    "for e in ids:\n",
    "    attributes.append(soup.select(\"#{} td\".format(e)))\n",
    "\n",
    "attributes[0]\n",
    "attributes2 = []\n",
    "for i in range(len(attributes)):\n",
    "    dictionary = {\n",
    "\n",
    "        \"datetime\": attributes[i][3].text.strip().replace(\"earthquake\", \"\").replace(\"\\xa0\",\"\").replace(\"min ago\", \"\"),\n",
    "        \"latitude\": attributes[i][4].text.strip(),\n",
    "        \"latitude2\": attributes[i][5].text.strip(),\n",
    "        \"longitude\": attributes[i][6].text.strip(),\n",
    "        \"longitude2\": attributes[i][7].text.strip(),\n",
    "        \"region_name\": attributes[i][11].text.strip()\n",
    "    }\n",
    "    \n",
    "    attributes2.append(dictionary)\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(attributes2)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the date, days, title, city, country of next 25 hackathon events as a Pandas dataframe table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://hackevents.co/hackathons'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count number of tweets by a given Twitter account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/pocholoficial?lang=es'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'241 Tweets'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# your code\n",
    "try:\n",
    "    res = requests.get(url)\n",
    "    html = res.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    a = soup.findAll(\"a\", class_=\"ProfileNav-stat\")\n",
    "    a=a[0].get(\"title\")\n",
    "    display(a)\n",
    "except:\n",
    "    print(\"Not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of followers of a given twitter account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** in case account/s name not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/pocholoficial?lang=es'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.275'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#your code\n",
    "res = requests.get(url)\n",
    "html = res.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "a = soup.findAll(\"span\", class_=\"ProfileNav-value\")\n",
    "display(a[2].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>English</td>\n",
       "      <td>5964000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Español</td>\n",
       "      <td>1554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>日本語</td>\n",
       "      <td>1175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Deutsch</td>\n",
       "      <td>2361000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Русский</td>\n",
       "      <td>1576000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Français</td>\n",
       "      <td>2152000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Italiano</td>\n",
       "      <td>1562000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>中文</td>\n",
       "      <td>1080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Português</td>\n",
       "      <td>1014000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Polski</td>\n",
       "      <td>1367000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0        1\n",
       "0    English  5964000\n",
       "1    Español  1554000\n",
       "2        日本語  1175000\n",
       "3    Deutsch  2361000\n",
       "4    Русский  1576000\n",
       "5   Français  2152000\n",
       "6   Italiano  1562000\n",
       "7         中文  1080000\n",
       "8  Português  1014000\n",
       "9     Polski  1367000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#your code\n",
    "res = requests.get(url)\n",
    "html = res.content\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "languages = soup.findAll(class_=\"central-featured-lang\")\n",
    "a = [[e.select(\"strong\")[0].text, e.select(\"bdi\")[0].text.replace(\"\\xa0\", \"\").replace(\"+\", \"\")] for e in languages]\n",
    "a = pd.DataFrame(a)\n",
    "display((a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Business and economy',\n",
       " 'Crime and justice',\n",
       " 'Defence',\n",
       " 'Education',\n",
       " 'Environment',\n",
       " 'Government',\n",
       " 'Government spending',\n",
       " 'Health',\n",
       " 'Mapping',\n",
       " 'Society',\n",
       " 'Towns and cities',\n",
       " 'Transport']"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "res = requests.get(url)\n",
    "html = res.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "datasets = soup.select(\"ol li h2\")\n",
    "\n",
    "[e.text for e in datasets]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 languages by number of native speakers stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Native Speakers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mandarin</td>\n",
       "      <td>918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hindi</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bengali</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Russian</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Western Punjabi</td>\n",
       "      <td>92.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Marathi</td>\n",
       "      <td>83.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Telugu</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Wu</td>\n",
       "      <td>81.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Turkish</td>\n",
       "      <td>79.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Korean</td>\n",
       "      <td>77.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>French</td>\n",
       "      <td>77.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>German</td>\n",
       "      <td>76.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Tamil</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Yue</td>\n",
       "      <td>73.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Urdu</td>\n",
       "      <td>68.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Javanese</td>\n",
       "      <td>68.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Italian</td>\n",
       "      <td>64.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Egyptian Arabic</td>\n",
       "      <td>64.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Gujarati</td>\n",
       "      <td>56.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Iranian Persian</td>\n",
       "      <td>52.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Bhojpuri</td>\n",
       "      <td>52.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Min Nan</td>\n",
       "      <td>50.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Hakka</td>\n",
       "      <td>48.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Jinyu</td>\n",
       "      <td>46.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Hausa</td>\n",
       "      <td>43.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Somali</td>\n",
       "      <td>16.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Malay</td>\n",
       "      <td>16.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Cebuano</td>\n",
       "      <td>15.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Nepali</td>\n",
       "      <td>15.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Mesopotamian Arabic</td>\n",
       "      <td>15.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Assamese</td>\n",
       "      <td>15.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Sinhala</td>\n",
       "      <td>15.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Northern Kurdish</td>\n",
       "      <td>14.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Hejazi Arabic</td>\n",
       "      <td>14.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Nigerian Fulfulde</td>\n",
       "      <td>14.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Bavarian</td>\n",
       "      <td>14.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>South Azerbaijani</td>\n",
       "      <td>13.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Greek</td>\n",
       "      <td>13.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Chittagonian</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Kazakh</td>\n",
       "      <td>12.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Deccan</td>\n",
       "      <td>12.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Hungarian</td>\n",
       "      <td>12.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Kinyarwanda</td>\n",
       "      <td>12.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Zulu</td>\n",
       "      <td>12.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>South Levantine Arabic</td>\n",
       "      <td>11.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Tunisian Arabic</td>\n",
       "      <td>11.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Sanaani Spoken Arabic</td>\n",
       "      <td>11.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Min Bei Chinese</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Southern Pashto</td>\n",
       "      <td>10.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Rundi</td>\n",
       "      <td>10.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Czech</td>\n",
       "      <td>10.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Taʽizzi-Adeni Arabic</td>\n",
       "      <td>10.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Uyghur</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Min Dong Chinese</td>\n",
       "      <td>10.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Sylheti</td>\n",
       "      <td>10.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Language Native Speakers\n",
       "0                 Mandarin             918\n",
       "1                  Spanish             480\n",
       "2                  English             379\n",
       "3                    Hindi             341\n",
       "4                  Bengali             228\n",
       "5               Portuguese             221\n",
       "6                  Russian             154\n",
       "7                 Japanese             128\n",
       "8          Western Punjabi            92.7\n",
       "9                  Marathi            83.1\n",
       "10                  Telugu            82.0\n",
       "11                      Wu            81.4\n",
       "12                 Turkish            79.4\n",
       "13                  Korean            77.3\n",
       "14                  French            77.2\n",
       "15                  German            76.1\n",
       "16              Vietnamese            76.0\n",
       "17                   Tamil            75.0\n",
       "18                     Yue            73.1\n",
       "19                    Urdu            68.6\n",
       "20                Javanese            68.3\n",
       "21                 Italian            64.8\n",
       "22         Egyptian Arabic            64.6\n",
       "23                Gujarati            56.4\n",
       "24         Iranian Persian            52.8\n",
       "25                Bhojpuri            52.2\n",
       "26                 Min Nan            50.1\n",
       "27                   Hakka            48.2\n",
       "28                   Jinyu            46.9\n",
       "29                   Hausa            43.9\n",
       "..                     ...             ...\n",
       "61                  Somali            16.2\n",
       "62                   Malay            16.1\n",
       "63                 Cebuano            15.9\n",
       "64                  Nepali            15.8\n",
       "65     Mesopotamian Arabic            15.7\n",
       "66                Assamese            15.3\n",
       "67                 Sinhala            15.3\n",
       "68        Northern Kurdish            14.6\n",
       "69           Hejazi Arabic            14.5\n",
       "70       Nigerian Fulfulde            14.5\n",
       "71                Bavarian            14.1\n",
       "72       South Azerbaijani            13.8\n",
       "73                   Greek            13.1\n",
       "74            Chittagonian            13.0\n",
       "75                  Kazakh            12.9\n",
       "76                  Deccan            12.8\n",
       "77               Hungarian            12.6\n",
       "78             Kinyarwanda            12.1\n",
       "79                    Zulu            12.1\n",
       "80  South Levantine Arabic            11.6\n",
       "81         Tunisian Arabic            11.6\n",
       "82   Sanaani Spoken Arabic            11.4\n",
       "83         Min Bei Chinese            11.0\n",
       "84         Southern Pashto            10.9\n",
       "85                   Rundi            10.8\n",
       "86                   Czech            10.7\n",
       "87    Taʽizzi-Adeni Arabic            10.5\n",
       "88                  Uyghur            10.4\n",
       "89        Min Dong Chinese            10.3\n",
       "90                 Sylheti            10.3\n",
       "\n",
       "[91 rows x 2 columns]"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "res = requests.get(url)\n",
    "html = res.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "tabla = soup.find_all(\"tbody\")[0]\n",
    "filas = tabla.find_all(\"tr\")[1:]\n",
    "tabla = []\n",
    "for fila in filas:\n",
    "    celdas = fila.find_all('td')\n",
    "    diccionario = {\n",
    "        \"Language\": celdas[1].find('a').text.strip(),\n",
    "        \"Native Speakers\": celdas[4].text.strip(),\n",
    "\n",
    "    }\n",
    "    tabla.append(diccionario)\n",
    "df = pd.DataFrame(tabla)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mandarin'"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code\n",
    "filas[0].find(\"a\").text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = city=input('Enter the city:')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Book name,price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
