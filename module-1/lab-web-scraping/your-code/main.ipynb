{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended contennt.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit each url and take a look at its source through Chrome DevTools. You'll need to identify the html tags, special class names etc. used for the html content you are expected to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are imported for you. If you prefer to use additional libraries feel free to uncomment them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "# from pprint import pprint\n",
    "# from lxml import html\n",
    "# from lxml.html import fromstring\n",
    "# import urllib.request\n",
    "# from urllib.request import urlopen\n",
    "# import random\n",
    "# import re\n",
    "# import scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "def ObtainSoup(url):\n",
    "    res = requests.get(url)\n",
    "    html = res.text\n",
    "    return BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "soup = ObtainSoup(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Emmanuel Garcia (blasten)',\n",
       " 'Hugo van Kemenade (hugovk)',\n",
       " 'Darío Kondratiuk (kblok)',\n",
       " 'Megan Marsh (SwampDragons)',\n",
       " 'Minko Gechev (mgechev)',\n",
       " 'William Falcon (williamFalcon)',\n",
       " 'Yoshifumi Kawai (neuecc)',\n",
       " 'Mateusz Burzyński (Andarist)',\n",
       " 'Sam Verschueren (SamVerschueren)',\n",
       " 'Whyrusleeping (whyrusleeping)',\n",
       " 'Holger Rapp (SirVer)',\n",
       " 'Francois Zaninotto (fzaninotto)',\n",
       " 'Dylan Vann (DylanVann)',\n",
       " 'Miek Gieben (miekg)',\n",
       " 'Richard Schneeman (schneems)',\n",
       " 'Alec Thomas (alecthomas)',\n",
       " '陈帅 (chenshuai2144)',\n",
       " 'François Beaufort (beaufortfrancois)',\n",
       " 'Tanner Linsley (tannerlinsley)',\n",
       " 'Erik Wijmans (erikwijmans)',\n",
       " 'Joshua Blum (joshblum)',\n",
       " 'Gabriel Aszalos (gbbr)',\n",
       " 'Rico Sta. Cruz (rstacruz)',\n",
       " 'Solly Ross (DirectXMan12)',\n",
       " 'Vinayak Mehta (vinayak-mehta)']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "developers = []\n",
    "\n",
    "for dev in soup.find_all('h1',class_='h3 lh-condensed'):\n",
    "    user = dev.find_all('a')[0]['href'].replace('/','')\n",
    "    name = dev.text.strip()\n",
    "    developers.append('{} ({})'.format(name, user))\n",
    "developers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t#### Display the trending Python repositories in GitHub\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'\n",
    "soup = ObtainSoup(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blasten',\n",
       " 'hugovk',\n",
       " 'kblok',\n",
       " 'SwampDragons',\n",
       " 'mgechev',\n",
       " 'williamFalcon',\n",
       " 'neuecc',\n",
       " 'Andarist',\n",
       " 'SamVerschueren',\n",
       " 'whyrusleeping',\n",
       " 'SirVer',\n",
       " 'fzaninotto',\n",
       " 'DylanVann',\n",
       " 'miekg',\n",
       " 'schneems',\n",
       " 'alecthomas',\n",
       " 'chenshuai2144',\n",
       " 'beaufortfrancois',\n",
       " 'tannerlinsley',\n",
       " 'erikwijmans',\n",
       " 'joshblum',\n",
       " 'gbbr',\n",
       " 'rstacruz',\n",
       " 'DirectXMan12',\n",
       " 'vinayak-mehta']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repos = []\n",
    "html = soup.find_all('h1','h3 lh-condensed')\n",
    "for r in html:\n",
    "    repos.append(r.find_all('a')[0]['href'].replace('/',''))\n",
    "repos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'\n",
    "soup = ObtainSoup(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/20px-Cscr-featured.svg.png',\n",
       " 'upload.wikimedia.org/wikipedia/en/thumb/1/1b/Semi-protection-shackle.svg/20px-Semi-protection-shackle.svg.png',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/8/87/Walt_Disney_1942_signature.svg/150px-Walt_Disney_1942_signature.svg.png',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Newman_Laugh-O-Gram_%281921%29.webm/220px-seek%3D2-Newman_Laugh-O-Gram_%281921%29.webm.jpg',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Trolley_Troubles_poster.jpg/170px-Trolley_Troubles_poster.jpg',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/7/71/Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg/170px-Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg',\n",
       " 'upload.wikimedia.org/wikipedia/en/thumb/4/4e/Steamboat-willie.jpg/170px-Steamboat-willie.jpg',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/5/57/Walt_Disney_1935.jpg/170px-Walt_Disney_1935.jpg',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg/220px-Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/1/15/Disney_drawing_goofy.jpg/170px-Disney_drawing_goofy.jpg',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/1/13/DisneySchiphol1951.jpg/220px-DisneySchiphol1951.jpg',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/8/8c/WaltDisneyplansDisneylandDec1954.jpg/220px-WaltDisneyplansDisneylandDec1954.jpg',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Walt_disney_portrait_right.jpg/170px-Walt_disney_portrait_right.jpg',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Walt_Disney_Grave.JPG/170px-Walt_Disney_Grave.JPG',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Roy_O._Disney_with_Company_at_Press_Conference.jpg/170px-Roy_O._Disney_with_Company_at_Press_Conference.jpg',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/a/a9/Disney_Display_Case.JPG/170px-Disney_Display_Case.JPG',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Disney1968.jpg/170px-Disney1968.jpg',\n",
       " 'upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/d/da/Animation_disc.svg/30px-Animation_disc.svg.png',\n",
       " 'upload.wikimedia.org/wikipedia/en/thumb/6/69/P_vip.svg/29px-P_vip.svg.png',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Magic_Kingdom_castle.jpg/24px-Magic_Kingdom_castle.jpg',\n",
       " 'upload.wikimedia.org/wikipedia/en/thumb/e/e7/Video-x-generic.svg/30px-Video-x-generic.svg.png',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Flag_of_Los_Angeles_County%2C_California.svg/30px-Flag_of_Los_Angeles_County%2C_California.svg.png',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Blank_television_set.svg/30px-Blank_television_set.svg.png',\n",
       " 'upload.wikimedia.org/wikipedia/en/thumb/a/a4/Flag_of_the_United_States.svg/30px-Flag_of_the_United_States.svg.png',\n",
       " 'upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/22px-Commons-logo.svg.png',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/25px-Wikiquote-logo.svg.png',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Wikidata-logo.svg/30px-Wikidata-logo.svg.png',\n",
       " 'upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n",
       " 'en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1',\n",
       " '/static/images/wikimedia-button.png',\n",
       " '/static/images/poweredby_mediawiki_88x31.png']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = []\n",
    "for image in soup.find_all('img'):\n",
    "    images.append(image['src'].replace('//',''))\n",
    "images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python' \n",
    "soup = ObtainSoup(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/w/index.php?title=Python&action=edit&section=1',\n",
       " '/wiki/Pythonidae',\n",
       " '/wiki/Python_(genus)',\n",
       " '/w/index.php?title=Python&action=edit&section=2',\n",
       " '/wiki/Python_(mythology)',\n",
       " '/wiki/Python_of_Aenus',\n",
       " '/wiki/Python_(painter)',\n",
       " '/wiki/Python_of_Byzantium',\n",
       " '/wiki/Python_of_Catana',\n",
       " '/w/index.php?title=Python&action=edit&section=3',\n",
       " '/wiki/Python_(film)',\n",
       " '/wiki/Pythons_2',\n",
       " '/wiki/Monty_Python',\n",
       " '/wiki/Python_(Monty)_Pictures',\n",
       " '/w/index.php?title=Python&action=edit&section=4',\n",
       " '/wiki/Python_(programming_language)',\n",
       " '/wiki/CPython',\n",
       " '/wiki/CMU_Common_Lisp',\n",
       " '/wiki/PERQ#PERQ_3',\n",
       " '/w/index.php?title=Python&action=edit&section=5',\n",
       " '/w/index.php?title=Python&action=edit&section=6',\n",
       " '/wiki/Python_(Busch_Gardens_Tampa_Bay)',\n",
       " '/wiki/Python_(Coney_Island,_Cincinnati,_Ohio)',\n",
       " '/wiki/Python_(Efteling)',\n",
       " '/w/index.php?title=Python&action=edit&section=7',\n",
       " '/wiki/Python_(automobile_maker)',\n",
       " '/wiki/Python_(Ford_prototype)',\n",
       " '/w/index.php?title=Python&action=edit&section=8',\n",
       " '/wiki/Colt_Python',\n",
       " '/wiki/Python_(missile)',\n",
       " '/wiki/Python_(nuclear_primary)',\n",
       " '/w/index.php?title=Python&action=edit&section=9',\n",
       " '/wiki/Python_Anghelo',\n",
       " '/w/index.php?title=Python&action=edit&section=10',\n",
       " '/wiki/PYTHON',\n",
       " '/w/index.php?title=Python&action=edit&section=11',\n",
       " '/wiki/Cython',\n",
       " '/wiki/Pyton',\n",
       " '/wiki/File:Disambig_gray.svg',\n",
       " '/wiki/Help:Disambiguation',\n",
       " '/wiki/Help:Category',\n",
       " '/wiki/Category:Disambiguation_pages',\n",
       " '/wiki/Category:Disambiguation_pages_with_short_description',\n",
       " '/wiki/Category:All_article_disambiguation_pages',\n",
       " '/wiki/Category:All_disambiguation_pages',\n",
       " '/wiki/Category:Animal_common_name_disambiguation_pages',\n",
       " '/wiki/Special:MyTalk',\n",
       " '/wiki/Special:MyContributions',\n",
       " '/w/index.php?title=Special:CreateAccount&returnto=Python',\n",
       " '/w/index.php?title=Special:UserLogin&returnto=Python',\n",
       " '/wiki/Python',\n",
       " '/wiki/Talk:Python',\n",
       " '/wiki/Python',\n",
       " '/w/index.php?title=Python&action=edit',\n",
       " '/w/index.php?title=Python&action=history',\n",
       " '/wiki/Main_Page',\n",
       " '/wiki/Main_Page',\n",
       " '/wiki/Portal:Contents',\n",
       " '/wiki/Portal:Featured_content',\n",
       " '/wiki/Portal:Current_events',\n",
       " '/wiki/Special:Random',\n",
       " '//shop.wikimedia.org',\n",
       " '/wiki/Help:Contents',\n",
       " '/wiki/Wikipedia:About',\n",
       " '/wiki/Wikipedia:Community_portal',\n",
       " '/wiki/Special:RecentChanges',\n",
       " '//en.wikipedia.org/wiki/Wikipedia:Contact_us',\n",
       " '/wiki/Special:WhatLinksHere/Python',\n",
       " '/wiki/Special:RecentChangesLinked/Python',\n",
       " '/wiki/Wikipedia:File_Upload_Wizard',\n",
       " '/wiki/Special:SpecialPages',\n",
       " '/w/index.php?title=Python&oldid=924004650',\n",
       " '/w/index.php?title=Python&action=info',\n",
       " '/w/index.php?title=Special:CiteThisPage&page=Python&id=924004650',\n",
       " '/w/index.php?title=Special:Book&bookcmd=book_creator&referer=Python',\n",
       " '/w/index.php?title=Special:ElectronPdf&page=Python&action=show-download-screen',\n",
       " '/w/index.php?title=Python&printable=yes',\n",
       " '//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License',\n",
       " '//creativecommons.org/licenses/by-sa/3.0/',\n",
       " '//foundation.wikimedia.org/wiki/Terms_of_Use',\n",
       " '//foundation.wikimedia.org/wiki/Privacy_policy',\n",
       " '//www.wikimediafoundation.org/',\n",
       " '/wiki/Wikipedia:About',\n",
       " '/wiki/Wikipedia:General_disclaimer',\n",
       " '//en.wikipedia.org/wiki/Wikipedia:Contact_us',\n",
       " '//en.m.wikipedia.org/w/index.php?title=Python&mobileaction=toggle_view_mobile']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links_html = soup.find_all('a',href=True)\n",
    "links = [l['href'] for l in links_html if l['href'].startswith('/')]\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Titles that have changed in the United States Code since its last release point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'\n",
    "soup = ObtainSoup(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Title 6 - Domestic Security', 'Title 31 - Money and Finance ٭']"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles_html = soup.find_all('div',{'class':'usctitlechanged'})\n",
    "titles = [t.text.strip() for t in titles_html]\n",
    "titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Python list with the top ten FBI's Most Wanted names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.fbi.gov/wanted/topten'\n",
    "soup = ObtainSoup(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['JASON DEREK BROWN',\n",
       " 'ALEXIS FLORES',\n",
       " 'EUGENE PALMER',\n",
       " 'SANTIAGO VILLALBA MEDEROS',\n",
       " 'RAFAEL CARO-QUINTERO',\n",
       " 'ROBERT WILLIAM FISHER',\n",
       " 'BHADRESHKUMAR CHETANBHAI PATEL',\n",
       " 'ARNOLDO JIMENEZ',\n",
       " 'ALEJANDRO ROSALES CASTILLO',\n",
       " 'YASER ABDEL SAID']"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wanted = soup.find_all('h3','title')\n",
    "topten = [e.find_all('a')[0].text for e in wanted]\n",
    "topten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'\n",
    "soup = ObtainSoup(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Region Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>20:24:36</td>\n",
       "      <td>35.83 N</td>\n",
       "      <td>17.66 W</td>\n",
       "      <td>CENTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>20:13:59</td>\n",
       "      <td>41.77 N</td>\n",
       "      <td>13.60 E</td>\n",
       "      <td>SOUTHERN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>20:00:51</td>\n",
       "      <td>23.22 S</td>\n",
       "      <td>68.90 W</td>\n",
       "      <td>ANTOFAGASTA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>19:54:14</td>\n",
       "      <td>35.71 N</td>\n",
       "      <td>17.56 W</td>\n",
       "      <td>SOUTHERN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>19:13:40</td>\n",
       "      <td>20.48 S</td>\n",
       "      <td>69.05 W</td>\n",
       "      <td>TARAPACA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>18:56:03</td>\n",
       "      <td>35.72 N</td>\n",
       "      <td>21.06 W</td>\n",
       "      <td>CENTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>18:52:18</td>\n",
       "      <td>37.89 N</td>\n",
       "      <td>12.48 W</td>\n",
       "      <td>UTAH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>18:38:27</td>\n",
       "      <td>46.30 N</td>\n",
       "      <td>7.37 E</td>\n",
       "      <td>III</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>18:35:26</td>\n",
       "      <td>46.32 N</td>\n",
       "      <td>7.35 E</td>\n",
       "      <td>SWITZERLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>18:25:00</td>\n",
       "      <td>16.36 S</td>\n",
       "      <td>72.82 W</td>\n",
       "      <td>NEAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>18:21:12</td>\n",
       "      <td>19.14 N</td>\n",
       "      <td>55.64 W</td>\n",
       "      <td>ISLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>18:16:30</td>\n",
       "      <td>11.30 N</td>\n",
       "      <td>86.32 W</td>\n",
       "      <td>NEAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>18:00:57</td>\n",
       "      <td>27.03 N</td>\n",
       "      <td>55.05 E</td>\n",
       "      <td>SOUTHERN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>17:59:30</td>\n",
       "      <td>8.64 N</td>\n",
       "      <td>82.81 W</td>\n",
       "      <td>PANAMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>17:54:30</td>\n",
       "      <td>11.49 N</td>\n",
       "      <td>85.98 W</td>\n",
       "      <td>NICARAGUA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>17:52:17</td>\n",
       "      <td>34.27 N</td>\n",
       "      <td>19.29 W</td>\n",
       "      <td>SANTA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>17:48:50</td>\n",
       "      <td>27.29 N</td>\n",
       "      <td>55.02 E</td>\n",
       "      <td>SOUTHERN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>17:47:53</td>\n",
       "      <td>41.77 N</td>\n",
       "      <td>13.61 E</td>\n",
       "      <td>SOUTHERN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>17:46:12</td>\n",
       "      <td>18.71 N</td>\n",
       "      <td>64.57 W</td>\n",
       "      <td>VIRGIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>17:35:21</td>\n",
       "      <td>41.78 N</td>\n",
       "      <td>13.61 E</td>\n",
       "      <td>SOUTHERN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>17:29:58</td>\n",
       "      <td>12.85 S</td>\n",
       "      <td>45.45 E</td>\n",
       "      <td>MAYOTTE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>17:18:00</td>\n",
       "      <td>12.41 S</td>\n",
       "      <td>77.18 W</td>\n",
       "      <td>NEAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>17:11:12</td>\n",
       "      <td>37.77 N</td>\n",
       "      <td>21.99 E</td>\n",
       "      <td>SOUTHERN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>16:50:25</td>\n",
       "      <td>31.78 S</td>\n",
       "      <td>71.30 W</td>\n",
       "      <td>COQUIMBO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>16:24:29</td>\n",
       "      <td>39.14 N</td>\n",
       "      <td>20.61 E</td>\n",
       "      <td>GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>15:40:53</td>\n",
       "      <td>43.93 N</td>\n",
       "      <td>11.59 E</td>\n",
       "      <td>CENTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>15:40:09</td>\n",
       "      <td>31.37 S</td>\n",
       "      <td>68.55 W</td>\n",
       "      <td>SAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>15:14:49</td>\n",
       "      <td>21.65 S</td>\n",
       "      <td>68.63 W</td>\n",
       "      <td>ANTOFAGASTA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>15:08:11</td>\n",
       "      <td>42.72 N</td>\n",
       "      <td>13.23 E</td>\n",
       "      <td>CENTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>15:01:10</td>\n",
       "      <td>42.73 N</td>\n",
       "      <td>13.25 E</td>\n",
       "      <td>CENTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>14:54:15</td>\n",
       "      <td>34.27 N</td>\n",
       "      <td>19.29 W</td>\n",
       "      <td>SANTA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>14:52:50</td>\n",
       "      <td>34.27 N</td>\n",
       "      <td>19.29 W</td>\n",
       "      <td>SANTA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>14:37:54</td>\n",
       "      <td>19.12 N</td>\n",
       "      <td>67.24 W</td>\n",
       "      <td>PUERTO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>14:36:33</td>\n",
       "      <td>27.15 N</td>\n",
       "      <td>55.04 E</td>\n",
       "      <td>SOUTHERN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>14:33:33</td>\n",
       "      <td>34.28 N</td>\n",
       "      <td>19.29 W</td>\n",
       "      <td>GREATER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>14:33:33</td>\n",
       "      <td>19.80 N</td>\n",
       "      <td>69.83 W</td>\n",
       "      <td>DOMINICAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>14:21:48</td>\n",
       "      <td>3.98 S</td>\n",
       "      <td>04.01 W</td>\n",
       "      <td>CENTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>13:34:38</td>\n",
       "      <td>59.02 N</td>\n",
       "      <td>52.18 W</td>\n",
       "      <td>SOUTHERN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>13:30:57</td>\n",
       "      <td>34.27 N</td>\n",
       "      <td>19.30 W</td>\n",
       "      <td>SANTA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>13:28:52</td>\n",
       "      <td>36.69 N</td>\n",
       "      <td>98.44 W</td>\n",
       "      <td>OKLAHOMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>13:25:35</td>\n",
       "      <td>46.26 N</td>\n",
       "      <td>7.01 E</td>\n",
       "      <td>SWITZERLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>13:19:44</td>\n",
       "      <td>34.27 N</td>\n",
       "      <td>19.29 W</td>\n",
       "      <td>SANTA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>13:16:11</td>\n",
       "      <td>12.97 N</td>\n",
       "      <td>89.06 W</td>\n",
       "      <td>OFFSHORE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>13:07:32</td>\n",
       "      <td>29.17 S</td>\n",
       "      <td>12.87 W</td>\n",
       "      <td>SOUTHERN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>13:05:39</td>\n",
       "      <td>34.28 N</td>\n",
       "      <td>19.29 W</td>\n",
       "      <td>GREATER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>12:59:43</td>\n",
       "      <td>38.13 N</td>\n",
       "      <td>38.77 E</td>\n",
       "      <td>EASTERN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>12:58:41</td>\n",
       "      <td>34.27 N</td>\n",
       "      <td>19.29 W</td>\n",
       "      <td>SANTA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>12:50:40</td>\n",
       "      <td>9.86 N</td>\n",
       "      <td>26.28 E</td>\n",
       "      <td>MINDANAO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>12:38:59</td>\n",
       "      <td>35.75 N</td>\n",
       "      <td>17.59 W</td>\n",
       "      <td>SOUTHERN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2019-11-07</td>\n",
       "      <td>12:02:16</td>\n",
       "      <td>19.16 N</td>\n",
       "      <td>55.46 W</td>\n",
       "      <td>ISLAND</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date      Time Latitude Longitude  Region Name\n",
       "0   2019-11-07  20:24:36  35.83 N   17.66 W      CENTRAL\n",
       "1   2019-11-07  20:13:59  41.77 N   13.60 E     SOUTHERN\n",
       "2   2019-11-07  20:00:51  23.22 S   68.90 W  ANTOFAGASTA\n",
       "3   2019-11-07  19:54:14  35.71 N   17.56 W     SOUTHERN\n",
       "4   2019-11-07  19:13:40  20.48 S   69.05 W     TARAPACA\n",
       "5   2019-11-07  18:56:03  35.72 N   21.06 W      CENTRAL\n",
       "6   2019-11-07  18:52:18  37.89 N   12.48 W         UTAH\n",
       "7   2019-11-07  18:38:27  46.30 N    7.37 E          III\n",
       "8   2019-11-07  18:35:26  46.32 N    7.35 E  SWITZERLAND\n",
       "9   2019-11-07  18:25:00  16.36 S   72.82 W         NEAR\n",
       "10  2019-11-07  18:21:12  19.14 N   55.64 W       ISLAND\n",
       "11  2019-11-07  18:16:30  11.30 N   86.32 W         NEAR\n",
       "12  2019-11-07  18:00:57  27.03 N   55.05 E     SOUTHERN\n",
       "13  2019-11-07  17:59:30   8.64 N   82.81 W       PANAMA\n",
       "14  2019-11-07  17:54:30  11.49 N   85.98 W    NICARAGUA\n",
       "15  2019-11-07  17:52:17  34.27 N   19.29 W        SANTA\n",
       "16  2019-11-07  17:48:50  27.29 N   55.02 E     SOUTHERN\n",
       "17  2019-11-07  17:47:53  41.77 N   13.61 E     SOUTHERN\n",
       "18  2019-11-07  17:46:12  18.71 N   64.57 W       VIRGIN\n",
       "19  2019-11-07  17:35:21  41.78 N   13.61 E     SOUTHERN\n",
       "20  2019-11-07  17:29:58  12.85 S   45.45 E      MAYOTTE\n",
       "21  2019-11-07  17:18:00  12.41 S   77.18 W         NEAR\n",
       "22  2019-11-07  17:11:12  37.77 N   21.99 E     SOUTHERN\n",
       "23  2019-11-07  16:50:25  31.78 S   71.30 W     COQUIMBO\n",
       "24  2019-11-07  16:24:29  39.14 N   20.61 E       GREECE\n",
       "25  2019-11-07  15:40:53  43.93 N   11.59 E      CENTRAL\n",
       "26  2019-11-07  15:40:09  31.37 S   68.55 W          SAN\n",
       "27  2019-11-07  15:14:49  21.65 S   68.63 W  ANTOFAGASTA\n",
       "28  2019-11-07  15:08:11  42.72 N   13.23 E      CENTRAL\n",
       "29  2019-11-07  15:01:10  42.73 N   13.25 E      CENTRAL\n",
       "30  2019-11-07  14:54:15  34.27 N   19.29 W        SANTA\n",
       "31  2019-11-07  14:52:50  34.27 N   19.29 W        SANTA\n",
       "32  2019-11-07  14:37:54  19.12 N   67.24 W       PUERTO\n",
       "33  2019-11-07  14:36:33  27.15 N   55.04 E     SOUTHERN\n",
       "34  2019-11-07  14:33:33  34.28 N   19.29 W      GREATER\n",
       "35  2019-11-07  14:33:33  19.80 N   69.83 W    DOMINICAN\n",
       "36  2019-11-07  14:21:48   3.98 S   04.01 W      CENTRAL\n",
       "37  2019-11-07  13:34:38  59.02 N   52.18 W     SOUTHERN\n",
       "38  2019-11-07  13:30:57  34.27 N   19.30 W        SANTA\n",
       "39  2019-11-07  13:28:52  36.69 N   98.44 W     OKLAHOMA\n",
       "40  2019-11-07  13:25:35  46.26 N    7.01 E  SWITZERLAND\n",
       "41  2019-11-07  13:19:44  34.27 N   19.29 W        SANTA\n",
       "42  2019-11-07  13:16:11  12.97 N   89.06 W     OFFSHORE\n",
       "43  2019-11-07  13:07:32  29.17 S   12.87 W     SOUTHERN\n",
       "44  2019-11-07  13:05:39  34.28 N   19.29 W      GREATER\n",
       "45  2019-11-07  12:59:43  38.13 N   38.77 E      EASTERN\n",
       "46  2019-11-07  12:58:41  34.27 N   19.29 W        SANTA\n",
       "47  2019-11-07  12:50:40   9.86 N   26.28 E     MINDANAO\n",
       "48  2019-11-07  12:38:59  35.75 N   17.59 W     SOUTHERN\n",
       "49  2019-11-07  12:02:16  19.16 N   55.46 W       ISLAND"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "date = []\n",
    "time = []\n",
    "longitude = []\n",
    "latitude = []\n",
    "region = []\n",
    "earthquakes = soup.find_all('tbody')[0].find_all('tr')\n",
    "for e in earthquakes:\n",
    "    e = e.text.replace('\\xa0', ' ')\n",
    "    date.append(re.findall('\\d{4}-\\d{2}-\\d{2}',e)[0])\n",
    "    time.append(re.findall('\\d{2}:\\d{2}:\\d{2}',e)[0])\n",
    "    latitude.append(re.findall('\\d{1,2}\\.\\d{2}\\s[A-Z]',e)[0])\n",
    "    longitude.append(re.findall('\\d{1,2}\\.\\d{2}\\s[A-Z]',e)[1])\n",
    "    region.append(re.findall('[A-Z]{3,}',e)[0])\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Date':date, 'Time':time, 'Latitude':latitude, \n",
    "    'Longitude':longitude, 'Region Name':region\n",
    "})\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the date, days, title, city, country of next 25 hackathon events as a Pandas dataframe table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://hackevents.co/hackathons'\n",
    "soup = ObtainSoup(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "hackatron = soup.find_all('div','card-body')[0]\n",
    "date = re.findall('\\d{2}/\\d{1}/\\d{4}',hackatron.find_all('p')[0].text)[0]\n",
    "for a in soup.find_all('a',href=True):\n",
    "    if a['href'].startswith('/event'):\n",
    "        new_url = a['href']\n",
    "#CANT BE DONE CITY NAME NOT ACCESSIBLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count number of tweets by a given Twitter account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/thedaiiypuppy'\n",
    "html = ObtainSoup(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.623 Tweets\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ntweets = html.find_all('div','ProfileNav')[0].find_all('a')[0]['title']\n",
    "    print(ntweets)\n",
    "except:\n",
    "    print('Account credentials not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of followers of a given twitter account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** in case account/s name not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385 mil\n"
     ]
    }
   ],
   "source": [
    "#your code\n",
    "try:\n",
    "    for li in html.find_all('div','ProfileNav')[0].find_all('li'):\n",
    "        if re.search('Seguidores', li.text):\n",
    "            print(li.text.strip().split('\\n')[2])\n",
    "except:\n",
    "    print('Account name not found')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'\n",
    "data = requests.get(url)\n",
    "soup = BeautifulSoup(data.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Number of articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>English</td>\n",
       "      <td>5 964 000+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Español</td>\n",
       "      <td>1 554 000+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nihongo</td>\n",
       "      <td>1 175 000+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Deutsch</td>\n",
       "      <td>2 361 000+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Russkiy</td>\n",
       "      <td>1 576 000+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Français</td>\n",
       "      <td>2 152 000+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Italiano</td>\n",
       "      <td>1 562 000+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Zhōngwén</td>\n",
       "      <td>1 080 000+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Português</td>\n",
       "      <td>1 014 000+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Polski</td>\n",
       "      <td>1 367 000+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Language Number of articles\n",
       "0    English         5 964 000+\n",
       "1    Español         1 554 000+\n",
       "2    Nihongo         1 175 000+\n",
       "3    Deutsch         2 361 000+\n",
       "4    Russkiy         1 576 000+\n",
       "5   Français         2 152 000+\n",
       "6   Italiano         1 562 000+\n",
       "7   Zhōngwén         1 080 000+\n",
       "8  Português         1 014 000+\n",
       "9     Polski         1 367 000+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#your code\n",
    "names = []\n",
    "articles = []\n",
    "for e in soup.find_all('div','central-featured')[0].find_all('a', title=True):\n",
    "    names.append(e['title'].split(' —')[0])\n",
    "    articles.append(e('bdi')[0].text)\n",
    "df = pd.DataFrame({\n",
    "    \"Language\": names,\n",
    "    \"Number of articles\": articles\n",
    "})\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'\n",
    "soup = ObtainSoup(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Business and economy',\n",
       " 'Crime and justice',\n",
       " 'Defence',\n",
       " 'Education',\n",
       " 'Environment',\n",
       " 'Government',\n",
       " 'Government spending',\n",
       " 'Health',\n",
       " 'Mapping',\n",
       " 'Society',\n",
       " 'Towns and cities',\n",
       " 'Transport']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code \n",
    "\n",
    "datasets_html = soup.select('#content')[0].find_all('h2')\n",
    "datasets = [d.text for d in datasets_html]\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 languages by number of native speakers stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'\n",
    "html = ObtainSoup(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mandarin (entire branch)',\n",
       " 'Spanish',\n",
       " 'English',\n",
       " 'Hindi[b]',\n",
       " 'Arabic',\n",
       " 'Portuguese',\n",
       " 'Bengali',\n",
       " 'Russian',\n",
       " 'Japanese',\n",
       " 'Punjabi']"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "table = html.find_all('table')[2].find_all('td')\n",
    "\n",
    "languages = [e.text for e in table if re.search('[a-z]',e.text)]\n",
    "languages[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/thedaiiypuppy'\n",
    "html = ObtainSoup(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grooming gone wrong.... she's still a cutie do :)pic.twitter.com/7RotCbb8YA\n",
      "Two sets of trouble :)pic.twitter.com/TO6VEx7TkH\n",
      "When you cant contain the laughter :)pic.twitter.com/9YBp6CLB0X\n",
      "I couldn't pick between the two, could you?pic.twitter.com/BKG5T116kU\n",
      "Booorriiiinggg :)pic.twitter.com/zEZhmtTVnn\n"
     ]
    }
   ],
   "source": [
    "# your code\n",
    "for tweet in html.find_all('div','js-tweet-text-container')[:5]:\n",
    "    print(tweet.text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'\n",
    "html = ObtainSoup(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie</th>\n",
       "      <th>Release Year</th>\n",
       "      <th>Movie Director</th>\n",
       "      <th>Movie stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cadena perpetua</td>\n",
       "      <td>1994</td>\n",
       "      <td>Frank Darabont</td>\n",
       "      <td>Tim Robbins, Morgan Freeman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>El padrino</td>\n",
       "      <td>1972</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>Marlon Brando, Al Pacino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El padrino: Parte II</td>\n",
       "      <td>1974</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>Al Pacino, Robert De Niro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>El caballero oscuro</td>\n",
       "      <td>2008</td>\n",
       "      <td>Christopher Nolan</td>\n",
       "      <td>Christian Bale, Heath Ledger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 hombres sin piedad</td>\n",
       "      <td>1957</td>\n",
       "      <td>Sidney Lumet</td>\n",
       "      <td>Henry Fonda, Lee J. Cobb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>Juego sucio</td>\n",
       "      <td>2002</td>\n",
       "      <td>Andrew Lau</td>\n",
       "      <td>Andy Lau, Tony Chiu-Wai Leung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>La batalla de Argel</td>\n",
       "      <td>1966</td>\n",
       "      <td>Gillo Pontecorvo</td>\n",
       "      <td>Brahim Hadjadj, Jean Martin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>La leyenda del pianista en el océano</td>\n",
       "      <td>1998</td>\n",
       "      <td>Giuseppe Tornatore</td>\n",
       "      <td>Tim Roth, Pruitt Taylor Vince</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>El castillo en el cielo</td>\n",
       "      <td>1986</td>\n",
       "      <td>Hayao Miyazaki</td>\n",
       "      <td>Anna Paquin, James Van Der Beek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Sholay</td>\n",
       "      <td>1975</td>\n",
       "      <td>Ramesh Sippy</td>\n",
       "      <td>Sanjeev Kumar, Dharmendra</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Movie Release Year         Movie Director  \\\n",
       "0                         Cadena perpetua         1994        Frank Darabont    \n",
       "1                              El padrino         1972  Francis Ford Coppola    \n",
       "2                    El padrino: Parte II         1974  Francis Ford Coppola    \n",
       "3                     El caballero oscuro         2008     Christopher Nolan    \n",
       "4                   12 hombres sin piedad         1957          Sidney Lumet    \n",
       "..                                    ...          ...                    ...   \n",
       "245                           Juego sucio         2002            Andrew Lau    \n",
       "246                   La batalla de Argel         1966      Gillo Pontecorvo    \n",
       "247  La leyenda del pianista en el océano         1998    Giuseppe Tornatore    \n",
       "248               El castillo en el cielo         1986        Hayao Miyazaki    \n",
       "249                                Sholay         1975          Ramesh Sippy    \n",
       "\n",
       "                         Movie stars  \n",
       "0        Tim Robbins, Morgan Freeman  \n",
       "1           Marlon Brando, Al Pacino  \n",
       "2          Al Pacino, Robert De Niro  \n",
       "3       Christian Bale, Heath Ledger  \n",
       "4           Henry Fonda, Lee J. Cobb  \n",
       "..                               ...  \n",
       "245    Andy Lau, Tony Chiu-Wai Leung  \n",
       "246      Brahim Hadjadj, Jean Martin  \n",
       "247    Tim Roth, Pruitt Taylor Vince  \n",
       "248  Anna Paquin, James Van Der Beek  \n",
       "249        Sanjeev Kumar, Dharmendra  \n",
       "\n",
       "[250 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# your code\n",
    "movies = []\n",
    "releases = []\n",
    "directors = []\n",
    "stars = []\n",
    "for movie in html.select('.titleColumn'):\n",
    "    staff = movie.find_all('a')[0]['title']\n",
    "    directors.append(staff.split('(dir.), ')[0])\n",
    "    stars.append(staff.split('(dir.), ')[1])\n",
    "    movies.append(movie.text.strip().split('\\n')[1].strip()) \n",
    "    releases.append(re.sub(r'\\D','',movie.text.strip().split('\\n')[2]))\n",
    "\n",
    "df_movie = pd.DataFrame({\n",
    "    'Movie':movies,\n",
    "    'Release Year':releases,\n",
    "    'Movie Director':directors,\n",
    "    'Movie stars':stars \n",
    "})\n",
    "display(df_movie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'select'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-c99aae854d0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#your code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0murl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'td'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'.titleColumn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'select'"
     ]
    }
   ],
   "source": [
    "#your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = city=input('Enter the city:')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Book name,price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
